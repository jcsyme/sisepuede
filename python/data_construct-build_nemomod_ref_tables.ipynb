{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:898: UserWarning: Invalid subsector attribute 'key_varreqs_partial'. Valid return type values are:'pycategory_primary', 'abv_subsector', 'sector', 'abv_sector', 'key_varreqs_all'\n",
      "  warnings.warn(f\"Invalid subsector attribute '{return_type}'. Valid return type values are:{valid_rts}\")\n",
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/data_structures.py:898: UserWarning: Invalid subsector attribute 'key_varreqs_partial'. Valid return type values are:'pycategory_primary', 'abv_subsector', 'sector', 'abv_sector', 'key_varreqs_all'\n",
      "  warnings.warn(f\"Invalid subsector attribute '{return_type}'. Valid return type values are:{valid_rts}\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'model_electricity' from '/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/model_electricity.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import model_attributes as ma\n",
    "from attribute_table import AttributeTable\n",
    "import model_afolu as mafl\n",
    "import model_ippu as mi\n",
    "import model_circular_economy as mc\n",
    "import model_energy as me\n",
    "import model_electricity as ml\n",
    "import model_socioeconomic as se\n",
    "from model_socioeconomic import Socioeconomic\n",
    "import setup_analysis as sa\n",
    "import support_functions as sf\n",
    "import importlib\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import sqlalchemy\n",
    "import sqlite3\n",
    "from typing import Union\n",
    "\n",
    "importlib.reload(ma)\n",
    "importlib.reload(sa)\n",
    "importlib.reload(sf)\n",
    "importlib.reload(mafl)\n",
    "importlib.reload(mc)\n",
    "importlib.reload(mi)\n",
    "importlib.reload(me)\n",
    "importlib.reload(se)\n",
    "importlib.reload(ml)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the base time slice attribute\n",
    "attr_time_slice = pd.read_csv(\"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/Energy/base_attribute_time_slice.csv\")\n",
    "attr_time_slice_cur = sa.model_attributes.get_other_attribute_table(\"time_slice\")\n",
    "# get the baseline specified demand profiles\n",
    "fp_spd_excel = \"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/Energy/estimate_of_nemomod_specified_demand_profile.xlsx\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##  get the hour from the name\n",
    "def get_hour(str_in: str) -> int:\n",
    "    if \"wd\" in str_in:\n",
    "        s = str_in.split(\"wd\")[1]\n",
    "    elif \"we\" in str_in:\n",
    "        s = str_in.split(\"we\")[1]\n",
    "    else:\n",
    "        s = - 1\n",
    "    \n",
    "    return int(s)\n",
    "\n",
    "##  auto-generate a description\n",
    "def format_desc(str_in: str, delta_hr: int = 6) -> str:\n",
    "    hr = get_hour(str_in)\n",
    "    if \"wd\" in str_in:\n",
    "        season = str_in.split(\"wd\")[0]\n",
    "        day = \"weekday\"\n",
    "    elif \"we\" in str_in:\n",
    "        season = str_in.split(\"we\")[0]\n",
    "        day = \"weekend\"\n",
    "    \n",
    "    hr0 = str((hr + 22)%24).rjust(2, \"0\")\n",
    "    hr1 = str((hr + 22 + delta_hr)%24).rjust(2, \"0\")\n",
    "    \n",
    "    str_out = f\"{season} {day}s from {hr0}:00 to {hr1}:00\"\n",
    "    \n",
    "    return str_out\n",
    "\n",
    "\n",
    "\n",
    "dict_rename_l = {}\n",
    "dict_hr_map = {}\n",
    "##  build a map for new hour sets\n",
    "for hr in range(24):\n",
    "    hr_shift = hr + 2\n",
    "    if hr_shift%24 < 6:\n",
    "        dict_hr_map.update({hr: 0})\n",
    "    elif hr_shift%24 < 12:\n",
    "        dict_hr_map.update({hr: 6})\n",
    "    elif hr_shift%24 < 18:\n",
    "        dict_hr_map.update({hr: 12})\n",
    "    else:\n",
    "        dict_hr_map.update({hr: 18})\n",
    "    \n",
    "# specify the l order \n",
    "dict_l_order_by_hr = {0: 1, 6: 2, 12: 3, 18: 4}\n",
    "\n",
    "##  build a map for time slice names\n",
    "for time_slice in list(set(attr_time_slice[\"time_slice\"])):\n",
    "    \n",
    "    hr = get_hour(time_slice)\n",
    "    time_slice_new = time_slice.replace(str(hr), str(dict_hr_map[hr]))\n",
    "    dict_rename_l.update({time_slice: time_slice_new})\n",
    "\n",
    "    \n",
    "##  perform aggregations on time slice attribute\n",
    "field_ts = \"time_slice\"\n",
    "field_ts_new = \"time_slice_new\"\n",
    "attr_time_slice[field_ts_new] = attr_time_slice[field_ts].replace(dict_rename_l)\n",
    "attr_time_slice[\"idn\"] = range(len(attr_time_slice))\n",
    "\n",
    "fields_grp = [field_ts_new, \"tg1\", \"tg2\"]\n",
    "fields_sum = [\"weight\"]\n",
    "dict_agg = dict(zip(fields_grp + [\"idn\"], [\"first\" for x in fields_grp + [\"idn\"]]))\n",
    "dict_agg.update(dict(zip(fields_sum, [\"sum\" for x in fields_sum])))\n",
    "attr_time_slice_new = attr_time_slice[list(dict_agg.keys())].groupby(\n",
    "    fields_grp\n",
    ").agg(\n",
    "    dict_agg\n",
    ").reset_index(\n",
    "    drop = True\n",
    ").sort_values(\n",
    "    by = [\"idn\"]\n",
    ").reset_index(\n",
    "    drop = True\n",
    ")\n",
    "\n",
    "attr_time_slice_new[\"hr\"] = attr_time_slice_new[\"time_slice_new\"].apply(get_hour)\n",
    "attr_time_slice_new[\"lorder\"] = attr_time_slice_new[\"hr\"].replace(dict_l_order_by_hr)\n",
    "attr_time_slice_new.drop([\"idn\", \"hr\"], axis = 1, inplace = True)\n",
    "attr_time_slice_new = attr_time_slice_new[[x for x in attr_time_slice.columns if x in attr_time_slice_new.columns]]\n",
    "attr_time_slice_new[\"description\"] = attr_time_slice_new[field_ts_new].apply(format_desc)\n",
    "attr_time_slice_new.rename(columns = {\"time_slice_new\": \"time_slice\"}, inplace = True)\n",
    "\n",
    "# order output\n",
    "attr_time_slice_new = attr_time_slice_new[\n",
    "    attr_time_slice_cur.table.columns\n",
    "].rename(columns = {\"time_slice\": \"``$TIME-SLICE$``\"})\n",
    "\n",
    "attr_time_slice_new.to_csv(attr_time_slice_cur.fp_table, index = None, encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "##  SpecifiedDemandProfile\n",
    "\n",
    "df_spd = pd.read_excel(fp_spd_excel, sheet_name = \"SpecifiedDemandProfile\")\n",
    "\n",
    "##  aggregate to new specification\n",
    "df_spd[field_ts_new] = df_spd[\"l\"].replace(dict_rename_l)\n",
    "fields_grp = [field_ts_new]\n",
    "fields_sum = [\"val\"]\n",
    "dict_agg = dict(zip(fields_grp + [\"id\"], [\"first\" for x in fields_grp + [\"id\"]]))\n",
    "dict_agg.update(dict(zip(fields_sum, [\"sum\" for x in fields_sum])))\n",
    "df_spd = df_spd[list(dict_agg.keys())].groupby(\n",
    "    fields_grp\n",
    ").agg(\n",
    "    dict_agg\n",
    ").reset_index(\n",
    "    drop = True\n",
    ").sort_values(\n",
    "    by = [\"id\"]\n",
    ").reset_index(\n",
    "    drop = True\n",
    ").rename(\n",
    "    columns = {field_ts_new: \"l\"}\n",
    ")\n",
    "\n",
    "\n",
    "# get regions, fuels, and time periods, then initialize\n",
    "attr_region = sa.model_attributes.get_other_attribute_table(sa.model_attributes.dim_region)\n",
    "regions = attr_region.key_values;\n",
    "df_spd_out = pd.DataFrame({\"r\": regions})\n",
    "# add dummy key to both and merge\n",
    "field_key_dummy = \"key_merge\"\n",
    "df_spd[field_key_dummy] = 0\n",
    "df_spd_out[field_key_dummy] = 0\n",
    "df_spd_out = pd.merge(df_spd_out, df_spd, on = [field_key_dummy], how = \"outer\").drop([field_key_dummy], axis = 1).sort_values(by = [\"r\", \"id\"])\n",
    "df_spd_out = df_spd_out.drop([\"id\"], axis = 1).reset_index(drop = True)\n",
    "\n",
    "\n",
    "# write to output\n",
    "fp_out = sa.dict_fp_csv_nemomod.get(sa.model_attributes.table_nemomod_specified_demand_profile)\n",
    "df_spd_out.to_csv(fp_out, index = None, encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check TimeSlice attribute\n",
    "attr_ts = sa.model_attributes.get_other_attribute_table(\"time_slice\")\n",
    "df_tmp = attr_ts.table.copy()\n",
    "\n",
    "\n",
    "dict_repl = {\"weekday\": 5/7, \"weekend\": 2/7}\n",
    "df_tmp[\"weight\"] = df_tmp[\"tg2\"].replace(dict_repl)\n",
    "\n",
    "dict_denom = {}\n",
    "for k in dict_repl.keys():\n",
    "    df_filt = df_tmp[df_tmp[\"tg2\"].isin([k])]\n",
    "    dict_denom.update({k: len(df_filt)})\n",
    "\n",
    "df_tmp[\"denom\"] = df_tmp[\"tg2\"].replace(dict_denom)\n",
    "df_tmp[\"weight\"] = np.array(df_tmp[\"weight\"])/np.array(df_tmp[\"denom\"])\n",
    "df_tmp.drop([\"denom\"], axis = 1, inplace = True)\n",
    "df_tmp.rename(columns = {\"time_slice\": \"``$TIME-SLICE$``\"}, inplace = True)\n",
    "# uncomment to export\n",
    "#df_tmp.to_csv(attr_ts.fp_table, index = None, encoding = \"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
